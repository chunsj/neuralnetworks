<!DOCTYPE html PUBLIC ""
    "">
<html><head><meta charset="UTF-8" /><link href="css/default.css" rel="stylesheet" type="text/css" /><script src="js/jquery.min.js" type="text/javascript"></script><script src="js/page_effects.js" type="text/javascript"></script><title>neuralnetworks.core documentation</title></head><body><div id="header"><h2>Generated by <a href="https://github.com/weavejester/codox">Codox</a></h2><h1><a href="index.html"><span class="project-title"><span class="project-name">Neuralnetworks</span> <span class="project-version">0.3.0-SNAPSHOT</span></span></a></h1></div><div class="sidebar primary"><h3 class="no-link"><span class="inner">Project</span></h3><ul class="index-link"><li class="depth-1 "><a href="index.html"><div class="inner">Index</div></a></li></ul><h3 class="no-link"><span class="inner">Namespaces</span></h3><ul><li class="depth-1"><div class="no-link"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>neuralnetworks</span></div></div></li><li class="depth-2 branch"><a href="neuralnetworks.bias-vector.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>bias-vector</span></div></a></li><li class="depth-2 branch"><a href="neuralnetworks.calculate.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>calculate</span></div></a></li><li class="depth-2 branch current"><a href="neuralnetworks.core.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>core</span></div></a></li><li class="depth-2 branch"><a href="neuralnetworks.error-fn.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>error-fn</span></div></a></li><li class="depth-2"><a href="neuralnetworks.optimizer.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>optimizer</span></div></a></li><li class="depth-3 branch"><a href="neuralnetworks.optimizer.gradient-descent.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>gradient-descent</span></div></a></li><li class="depth-3"><a href="neuralnetworks.optimizer.stopping-conditions.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>stopping-conditions</span></div></a></li><li class="depth-2 branch"><a href="neuralnetworks.sigmoid-fn.html"><div class="inner"><span class="tree" style="top: -83px;"><span class="top" style="height: 92px;"></span><span class="bottom"></span></span><span>sigmoid-fn</span></div></a></li><li class="depth-2"><a href="neuralnetworks.utils.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>utils</span></div></a></li></ul></div><div class="sidebar secondary"><h3><a href="#top"><span class="inner">Public Vars</span></a></h3><ul><li class="depth-1"><a href="neuralnetworks.core.html#var-error"><div class="inner"><span>error</span></div></a></li><li class="depth-1"><a href="neuralnetworks.core.html#var-new-instance"><div class="inner"><span>new-instance</span></div></a></li><li class="depth-1"><a href="neuralnetworks.core.html#var-predict"><div class="inner"><span>predict</span></div></a></li><li class="depth-1"><a href="neuralnetworks.core.html#var-randomize-thetas"><div class="inner"><span>randomize-thetas</span></div></a></li><li class="depth-1"><a href="neuralnetworks.core.html#var-train.21"><div class="inner"><span>train!</span></div></a></li></ul></div><div class="namespace-docs" id="content"><h1 class="anchor" id="top">neuralnetworks.core</h1><div class="doc"><div class="markdown"></div></div><div class="public anchor" id="var-error"><h3>error</h3><div class="usage"><code>(error instance input expected-output)</code></div><div class="doc"><div class="markdown"><p>Calculate the error between the expected output node and the predicted nodes</p></div></div><div class="src-link"><a href="https://github.com/ronaldsuwandi/neuralnetworks/blob/master/src/neuralnetworks/core.clj#L50">view source</a></div></div><div class="public anchor" id="var-new-instance"><h3>new-instance</h3><div class="usage"><code>(new-instance input thetas output problem-type)</code><code>(new-instance input thetas output problem-type options)</code></div><div class="doc"><div class="markdown"><p>Creates new instance of neural networks.</p>
<p>Problem-type accepts either <code>:classification</code> or <code>:regression</code>. Problem-type determines the default sigmoid and error function</p>
<p>For <code>classification</code> it will use</p>
<pre><code>{:sigmoid-fn standard-logistic
 :error-fn cross-entropy}
</code></pre>
<p>for the options. Cross-entropy is more suitable because it penalizes misclassification</p>
<p>Otherwise, for <code>:regression</code> it will use</p>
<pre><code>{:sigmoid-fn hyperbolic-tangent
 :error-fn mean-squared-error}
</code></pre>
<p>Mean squared error is best suited for regression (curve-fitting) problem</p>
<p>Options will be a hash map of</p>
<pre><code>{:regularization-rate value
 :activation-fn function
 :sigmoid-fn function       ; optional, if you want to customize/override sigmoid function
 :error-fn function         ; optional, if you want to customize/override error function
 :optimizer optimizer}
</code></pre>
<p>Thetas would be the vector of initial weights matrices between each layer. To create a single hidden layer, Thetas would be a vector of two weight matrices.</p>
<p>If optimizer is not specified, by default it will use gradient descent optimizer with the following settings:</p>
<ul>
  <li>initial learning rate of 4</li>
  <li>learning rate update of 0.5</li>
</ul>
<p><em>Note</em> it is important to always normalize the input and output nodes for better performance</p>
<p>Returns a hashmap</p>
<pre><code>{
  :input input-matrix
  :output output-matrix
  :regularization-rate value ; default is 0
  :sigmoid-fn function       ; default is standard logistic for classification, hyperbolic
                             ; tangent for regression
  :errror-fn function        ; default is cross-entropy for classification, mean squared error
                             ; for regression
  :optimizer function        ; default is gradient-descent with the default options
  :states {
            :thetas [theta-matrix-1, theta-matrix-2, ...]
            :iteration (atom 0)
            :error (atom nil)
            :training-durations 1 ; in ms
          }
}
</code></pre></div></div><div class="src-link"><a href="https://github.com/ronaldsuwandi/neuralnetworks/blob/master/src/neuralnetworks/core.clj#L93">view source</a></div></div><div class="public anchor" id="var-predict"><h3>predict</h3><div class="usage"><code>(predict instance input)</code></div><div class="doc"><div class="markdown"><p>Predict the output given the neural networks settings</p></div></div><div class="src-link"><a href="https://github.com/ronaldsuwandi/neuralnetworks/blob/master/src/neuralnetworks/core.clj#L42">view source</a></div></div><div class="public anchor" id="var-randomize-thetas"><h3>randomize-thetas</h3><div class="usage"><code>(randomize-thetas input-nodes hidden-layers-nodes output-nodes)</code></div><div class="doc"><div class="markdown"><p>Create a randomize thetas for initial values</p>
<p>It will the following formula</p>
<p><code>randomize(L0, L1) * 2 * epsilon - epsilon</code></p>
<p>Where L0 and L1 are the number of nodes adjacent to theta (e.g. input-node and hidden-layer-1-nodes, hidden-layer-1-nodes and output-nodes)</p>
<p>Epsilon will be calculated using the following formula</p>
<p><code>sqrt(6) / sqrt(L0 + L1)</code></p>
<p>hidden-layers-nodes will be a vector of integers (number of nodes per hidden layer)</p></div></div><div class="src-link"><a href="https://github.com/ronaldsuwandi/neuralnetworks/blob/master/src/neuralnetworks/core.clj#L65">view source</a></div></div><div class="public anchor" id="var-train.21"><h3>train!</h3><div class="usage"><code>(train! instance stopping-conditions)</code></div><div class="doc"><div class="markdown"><p>Train the neural networks. This will update the thetas/weights</p>
<p>Stopping conditions is a vector of stopping condition functions used by the optimizer which in turn used by neural networks training function.</p>
<p>If multiple stopping conditions are provided, it will be treated as <em>OR</em> meaning as long as one of the condition is satisfied, training will be stopped (i.e. optimizer is finished)</p></div></div><div class="src-link"><a href="https://github.com/ronaldsuwandi/neuralnetworks/blob/master/src/neuralnetworks/core.clj#L11">view source</a></div></div></div></body></html>