<!DOCTYPE html PUBLIC ""
    "">
<html><head><meta charset="UTF-8" /><link href="css/default.css" rel="stylesheet" type="text/css" /><script src="js/jquery.min.js" type="text/javascript"></script><script src="js/page_effects.js" type="text/javascript"></script><title>neuralnetworks.optimizer.gradient-descent documentation</title></head><body><div id="header"><h2>Generated by <a href="https://github.com/weavejester/codox">Codox</a></h2><h1><a href="index.html"><span class="project-title"><span class="project-name">Neuralnetworks</span> <span class="project-version">0.3.0-SNAPSHOT</span></span></a></h1></div><div class="sidebar primary"><h3 class="no-link"><span class="inner">Project</span></h3><ul class="index-link"><li class="depth-1 "><a href="index.html"><div class="inner">Index</div></a></li></ul><h3 class="no-link"><span class="inner">Namespaces</span></h3><ul><li class="depth-1"><div class="no-link"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>neuralnetworks</span></div></div></li><li class="depth-2 branch"><a href="neuralnetworks.bias-vector.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>bias-vector</span></div></a></li><li class="depth-2 branch"><a href="neuralnetworks.calculate.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>calculate</span></div></a></li><li class="depth-2 branch"><a href="neuralnetworks.core.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>core</span></div></a></li><li class="depth-2 branch"><a href="neuralnetworks.error-fn.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>error-fn</span></div></a></li><li class="depth-2"><a href="neuralnetworks.optimizer.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>optimizer</span></div></a></li><li class="depth-3 branch current"><a href="neuralnetworks.optimizer.gradient-descent.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>gradient-descent</span></div></a></li><li class="depth-3"><a href="neuralnetworks.optimizer.stopping-conditions.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>stopping-conditions</span></div></a></li><li class="depth-2 branch"><a href="neuralnetworks.sigmoid-fn.html"><div class="inner"><span class="tree" style="top: -83px;"><span class="top" style="height: 92px;"></span><span class="bottom"></span></span><span>sigmoid-fn</span></div></a></li><li class="depth-2"><a href="neuralnetworks.utils.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>utils</span></div></a></li></ul></div><div class="sidebar secondary"><h3><a href="#top"><span class="inner">Public Vars</span></a></h3><ul><li class="depth-1"><a href="neuralnetworks.optimizer.gradient-descent.html#var-gradient-descent"><div class="inner"><span>gradient-descent</span></div></a></li><li class="depth-1"><a href="neuralnetworks.optimizer.gradient-descent.html#var-line-search"><div class="inner"><span>line-search</span></div></a></li><li class="depth-1"><a href="neuralnetworks.optimizer.gradient-descent.html#var-update-thetas"><div class="inner"><span>update-thetas</span></div></a></li></ul></div><div class="namespace-docs" id="content"><h1 class="anchor" id="top">neuralnetworks.optimizer.gradient-descent</h1><div class="doc"><div class="markdown"></div></div><div class="public anchor" id="var-gradient-descent"><h3>gradient-descent</h3><div class="usage"><code>(gradient-descent initial-learning-rate learning-rate-update-rate)</code></div><div class="doc"><div class="markdown"><p>Creates new instance of gradient descent optimizer. It uses Backtracking Line Search to find the good value of learning-rate (alpha) to allows it converge faster</p>
<p>To disable backtracking line search, simply set the learning-rate-update-rate to 1.0</p>
<p>Learning-rate-update-rate must be between (0, 1]</p>
<p>Cost function must returns both <code>gradients</code> and <code>cost</code> value</p>
<pre><code>{:cost 1.5142
 :gradients [1.2 -0.5]}
</code></pre></div></div><div class="src-link"><a href="https://github.com/ronaldsuwandi/neuralnetworks/blob/master/src/neuralnetworks/optimizer/gradient_descent.clj#L64">view source</a></div></div><div class="public anchor" id="var-line-search"><h3>line-search</h3><div class="usage"><code>(line-search cost-fn init-alpha beta thetas theta-gradients)</code></div><div class="doc"><div class="markdown"><p>Uses backtrack line search algorithm to find the best learning-rate (alpha) value.</p>
<p>Backtrack will stop if either of the following conditions are met:</p>
<ul>
  <li>Cost of new theta (updated theta - i.e. theta is updated with alpha and gradients) is less or  equals to cost of the original theta minus gradient length squared with alpha</li>
  <li>Approximately alpha reaches almost zero (1e-10)</li>
  <li>Approximately all gradients almost zero (1e-10)</li>
</ul>
<p>Reference: <a href="https://www.cs.cmu.edu/~ggordon/10725-F12/slides/05-gd-revisited.pdf">Gradient Descent Revisited</a></p></div></div><div class="src-link"><a href="https://github.com/ronaldsuwandi/neuralnetworks/blob/master/src/neuralnetworks/optimizer/gradient_descent.clj#L13">view source</a></div></div><div class="public anchor" id="var-update-thetas"><h3>update-thetas</h3><div class="usage"><code>(update-thetas thetas theta-gradients alpha)</code></div><div class="doc"><div class="markdown"><p>Updates the thetas (weight) based on the provided gradients and alpha. Gradients must be the same dimension as thetas</p></div></div><div class="src-link"><a href="https://github.com/ronaldsuwandi/neuralnetworks/blob/master/src/neuralnetworks/optimizer/gradient_descent.clj#L7">view source</a></div></div></div></body></html>